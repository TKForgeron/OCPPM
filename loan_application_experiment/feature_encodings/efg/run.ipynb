{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/Development/OCPPM/.env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 25\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtensorboard\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m# import utils\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[39m# from dotenv import load_dotenv\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[39m# Custom imports\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mconfig\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfiles\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbpi17\u001b[39;00m \u001b[39mimport\u001b[39;00m bpi17_config\n\u001b[1;32m     26\u001b[0m \u001b[39m# from importing_ocel import build_feature_storage, load_ocel, pickle_feature_storage\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgeometric_models\u001b[39;00m \u001b[39mimport\u001b[39;00m AdamsGCN, GraphModel\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'config'"
     ]
    }
   ],
   "source": [
    "# Python native\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from copy import copy\n",
    "from datetime import datetime\n",
    "from statistics import median as median\n",
    "from sys import platform\n",
    "from typing import Any, Callable\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import ocpa.algo.predictive_monitoring.factory as feature_factory\n",
    "\n",
    "# PyG\n",
    "import torch\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "import torch.utils.tensorboard\n",
    "# import utils\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# Custom imports\n",
    "from config.files.bpi17 import bpi17_config\n",
    "# from importing_ocel import build_feature_storage, load_ocel, pickle_feature_storage\n",
    "from models.geometric_models import AdamsGCN, GraphModel\n",
    "from experiment.feature_encodings.efg.efg import EFG\n",
    "\n",
    "# Object centric process mining\n",
    "from ocpa.algo.predictive_monitoring.obj import Feature_Storage as FeatureStorage\n",
    "\n",
    "# # Simple machine learning models, procedure tools, and evaluation metrics\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_experiment_configuration(\n",
    "    parameters: dict, verbose: bool = False\n",
    ") -> tuple[\n",
    "    dict[str, Any],\n",
    "    dict[str, Any],\n",
    "    str,\n",
    "    str,\n",
    "    str,\n",
    "    str,\n",
    "    tuple[str, tuple],\n",
    "    str,\n",
    "    dict[str, Any],\n",
    "    int,\n",
    "    int,\n",
    "    int,\n",
    "    int,\n",
    "    torch.Generator,\n",
    "    Callable[[int], None],\n",
    "    torch.device,\n",
    "    bool,\n",
    "]:\n",
    "    load_dotenv(\".env_secrets\")\n",
    "    if platform == \"linux\" or platform == \"linux2\":\n",
    "        LOCAL_ROOT_DIR = os.getenv(\"lin_root_dir\")\n",
    "    elif platform == \"win32\":\n",
    "        LOCAL_ROOT_DIR = os.getenv(\"win_root_dir\")\n",
    "    else:\n",
    "        # We don't do apple\n",
    "        raise RuntimeError(\"Cannot determine operating system\")\n",
    "\n",
    "    # Configure whether to run the training loop again\n",
    "    # Define where to find previously trained best model\n",
    "    LOAD_PREVIOUS_RUN = {\n",
    "        \"run_train_loop_again\": False,\n",
    "        \"state_dict_dir\": f\"{LOCAL_ROOT_DIR}/models/AdamsGCN_20230321_12h34m\",\n",
    "    }\n",
    "\n",
    "    (\n",
    "        STORAGE_PATH,\n",
    "        SPLIT_FEATURE_STORAGE_FILE,\n",
    "        RAW_FEATURE_STORAGE_FILE,\n",
    "        TARGET_LABEL,\n",
    "        SUBGRAPH_SIZE,\n",
    "        EPOCHS,\n",
    "        BATCH_SIZE,\n",
    "        RANDOM_SEED,\n",
    "        ORIGINAL_LOG_FILE,\n",
    "        FEATURE_SET,\n",
    "        *rest,\n",
    "    ) = parameters.values()\n",
    "    LOG_PARAMETERS = None\n",
    "    if rest:\n",
    "        if type(rest[0]) == dict:\n",
    "            LOG_PARAMETERS = rest[0]\n",
    "\n",
    "    # Initializing random seeds for maximizing reproducibility\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    random.seed(RANDOM_SEED)\n",
    "\n",
    "    def seed_worker(worker_id: int) -> None:\n",
    "        # worker_seed = torch.initial_seed() % RANDOM_SEED\n",
    "        worker_seed = RANDOM_SEED\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    generator = torch.Generator().manual_seed(RANDOM_SEED)\n",
    "    # torch.use_deterministic_algorithms(True) # incompatible with GCN\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if LOG_PARAMETERS:\n",
    "        print(f\"LOG_PARAMETERS: {LOG_PARAMETERS}\")\n",
    "    print(f\"ORIGINAL_LOG_FILE: {ORIGINAL_LOG_FILE}\")\n",
    "    print(f\"STORAGE_PATH: {STORAGE_PATH}\")\n",
    "    print(f\"SPLIT_FEATURE_STORAGE_FILE: {SPLIT_FEATURE_STORAGE_FILE}\")\n",
    "    print(f\"RAW_FEATURE_STORAGE_FILE: {RAW_FEATURE_STORAGE_FILE}\")\n",
    "    print(f\"TARGET_LABEL: {TARGET_LABEL}\")\n",
    "    print(f\"LOCAL_ROOT_DIR: {LOCAL_ROOT_DIR}\")\n",
    "    print(f\"SUBGRAPH_SIZE: {SUBGRAPH_SIZE}\")\n",
    "    print(f\"RANDOM_SEED: {RANDOM_SEED}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"EPOCHS: {EPOCHS}\")\n",
    "    print(f\"BATCH_SIZE: {BATCH_SIZE}\")\n",
    "    # Verbose determines whether to print anything when running the full script (this config is always printed)\n",
    "    print(f\"verbose: {verbose}\")\n",
    "    print()\n",
    "\n",
    "    return (\n",
    "        FEATURE_SET,\n",
    "        LOG_PARAMETERS,\n",
    "        ORIGINAL_LOG_FILE,\n",
    "        STORAGE_PATH,\n",
    "        SPLIT_FEATURE_STORAGE_FILE,\n",
    "        RAW_FEATURE_STORAGE_FILE,\n",
    "        TARGET_LABEL,\n",
    "        LOCAL_ROOT_DIR,\n",
    "        LOAD_PREVIOUS_RUN,\n",
    "        RANDOM_SEED,\n",
    "        SUBGRAPH_SIZE,\n",
    "        EPOCHS,\n",
    "        BATCH_SIZE,\n",
    "        generator,\n",
    "        seed_worker,\n",
    "        device,\n",
    "        verbose,\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_feature_storage(\n",
    "    original_log_file: str,\n",
    "    log_parameters: dict[str, Any],\n",
    "    feature_set: dict[str, Any],\n",
    "    storage_path: str,\n",
    "    split_feature_storage_file: str,\n",
    "    raw_feature_storage_file: str,\n",
    "    seed: int,\n",
    ") -> None:\n",
    "    # If SPLIT_FEATURE_STORAGE_FILE not cached, generate it\n",
    "    if not os.path.exists(f\"{storage_path}/raw/{split_feature_storage_file}\"):\n",
    "        if not os.path.exists(f\"{storage_path}/raw/{raw_feature_storage_file}\"):\n",
    "            ocel = load_ocel(original_log_file, log_parameters)\n",
    "            act_features = []\n",
    "            if \"C2\" in feature_set.keys():\n",
    "                activities = ocel.log.log[\"event_activity\"].unique().tolist()\n",
    "                del feature_set[\"C2\"]\n",
    "                act_features = [\n",
    "                    (feature_factory.EVENT_PRECEDING_ACTIVITIES, (act,))\n",
    "                    for act in activities\n",
    "                ]\n",
    "            feature_list = act_features + list(feature_set.values())\n",
    "            feature_storage = build_feature_storage(ocel, feature_list)\n",
    "            pickle_feature_storage(\n",
    "                feature_storage, f\"{storage_path}/raw/{raw_feature_storage_file}\"\n",
    "            )\n",
    "        else:\n",
    "            with open(f\"{storage_path}/raw/{raw_feature_storage_file}\", \"rb\") as file:\n",
    "                feature_storage: FeatureStorage = pickle.load(file)\n",
    "\n",
    "        # Adams didn't give this split a random seed,\n",
    "        # thus we can split the validation set in this arbitrary manner\n",
    "        feature_storage.extract_normalized_train_test_split(\n",
    "            test_size=0.3,\n",
    "            validation_size=0.7 * 0.2,\n",
    "            scaler=StandardScaler,\n",
    "            scaling_exempt_features=[],\n",
    "            state=seed,\n",
    "        )\n",
    "\n",
    "        with open(\n",
    "            f\"{storage_path}/raw/{split_feature_storage_file}\",\n",
    "            \"wb\",\n",
    "        ) as file:\n",
    "            pickle.dump(feature_storage, file)\n",
    "\n",
    "\n",
    "def load_datasets(\n",
    "    storage_path: str,\n",
    "    split_feature_storage_file: str,\n",
    "    target_label: tuple[str, tuple],\n",
    "    train: bool = True,\n",
    "    val: bool = True,\n",
    "    test: bool = True,\n",
    ") -> list[EventSubGraphDataset]:\n",
    "    datasets = []\n",
    "    if train:\n",
    "        ds_train = EventSubGraphDataset(\n",
    "            train=True,\n",
    "            root=storage_path,\n",
    "            filename=split_feature_storage_file,\n",
    "            label_key=target_label,\n",
    "            size_subgraph_samples=SUBGRAPH_SIZE,\n",
    "            verbosity=51,\n",
    "        )\n",
    "        datasets.append(ds_train)\n",
    "    if val:\n",
    "        ds_val = EventSubGraphDataset(\n",
    "            validation=True,\n",
    "            root=storage_path,\n",
    "            filename=split_feature_storage_file,\n",
    "            label_key=target_label,\n",
    "            size_subgraph_samples=SUBGRAPH_SIZE,\n",
    "            verbosity=51,\n",
    "        )\n",
    "        datasets.append(ds_val)\n",
    "    if test:\n",
    "        ds_test = EventSubGraphDataset(\n",
    "            test=True,\n",
    "            root=storage_path,\n",
    "            filename=split_feature_storage_file,\n",
    "            label_key=target_label,\n",
    "            size_subgraph_samples=SUBGRAPH_SIZE,\n",
    "            verbosity=51,\n",
    "        )\n",
    "        datasets.append(ds_test)\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def print_dataset_summaries(\n",
    "    ds_train: EventSubGraphDataset,\n",
    "    ds_val: EventSubGraphDataset,\n",
    "    ds_test: EventSubGraphDataset,\n",
    ") -> None:\n",
    "    print(\"Train set\")\n",
    "    print(ds_train.get_summary(), \"\\n\")\n",
    "    print(\"Validation set\")\n",
    "    print(ds_val.get_summary(), \"\\n\")\n",
    "    print(\"Test set\")\n",
    "    print(ds_test.get_summary(), \"\\n\")\n",
    "\n",
    "\n",
    "def configure_model(\n",
    "    num_node_features: int,\n",
    "    num_hidden_features: int,\n",
    "    size_subgraph_samples: int,\n",
    "    device: torch.device,\n",
    ") -> GraphModel:\n",
    "    # Initialize model\n",
    "    model = AdamsGCN(\n",
    "        num_node_features=num_node_features,\n",
    "        hyperparams={\n",
    "            \"num_hidden_features\": num_hidden_features,\n",
    "            \"size_subgraph_samples\": size_subgraph_samples,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    model = model.to(device)\n",
    "    # data = ds_train.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "def count_parameters(model: GraphModel) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def prepare_dataloaders(\n",
    "    batch_size: int,\n",
    "    ds_train: EventSubGraphDataset = None,\n",
    "    ds_val: EventSubGraphDataset = None,\n",
    "    ds_test: EventSubGraphDataset = None,\n",
    "    shuffle: bool = True,\n",
    "    pin_memory: bool = True,\n",
    "    num_workers: int = 4,\n",
    "    seed_worker: Callable[[int], None] = None,\n",
    "    generator: torch.Generator = None,\n",
    ") -> list[DataLoader]:\n",
    "    dataloaders = []\n",
    "    if ds_train:\n",
    "        train_loader = DataLoader(\n",
    "            ds_train,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            pin_memory=pin_memory,\n",
    "            num_workers=num_workers,\n",
    "            worker_init_fn=seed_worker,\n",
    "            generator=generator,\n",
    "        )\n",
    "        dataloaders.append(train_loader)\n",
    "    if ds_val:\n",
    "        val_loader = DataLoader(\n",
    "            ds_val,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            pin_memory=pin_memory,\n",
    "            num_workers=num_workers,\n",
    "            worker_init_fn=seed_worker,\n",
    "            generator=generator,\n",
    "        )\n",
    "        dataloaders.append(val_loader)\n",
    "    if ds_test:\n",
    "        test_loader = DataLoader(\n",
    "            ds_test,\n",
    "            batch_size=128,\n",
    "            shuffle=shuffle,\n",
    "            pin_memory=pin_memory,\n",
    "            num_workers=num_workers,\n",
    "            worker_init_fn=seed_worker,\n",
    "            generator=generator,\n",
    "        )\n",
    "        dataloaders.append(test_loader)\n",
    "    return dataloaders\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    epoch_index: int,\n",
    "    model: GraphModel,\n",
    "    train_loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
    "    tb_writer: SummaryWriter,\n",
    "    device: torch.device,\n",
    "    verbose: bool = True,\n",
    ") -> float:\n",
    "    if verbose:\n",
    "        print(f\"EPOCH {epoch_index + 1}:\")\n",
    "\n",
    "    # Enumerate over the data\n",
    "    running_loss = 0.0\n",
    "    last_loss = 0\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        # Use GPU\n",
    "        batch.to(device)\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, adjacency_matrix, labels = (\n",
    "            batch.x.float(),  # k times the batch_size, where k is the subgraph size\n",
    "            batch.edge_index,\n",
    "            batch.y.float(),\n",
    "        )\n",
    "        # Reset gradients (set_to_none is faster than to zero)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        # Passing the node features and the connection info\n",
    "        outputs = model(inputs, adjacency_matrix)\n",
    "        # Compute loss and gradients\n",
    "        loss = loss_fn(torch.squeeze(outputs), labels)\n",
    "        loss.backward()\n",
    "        # Adjust learnable weights\n",
    "        optimizer.step()\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000  # loss per batch\n",
    "            if verbose:\n",
    "                print(f\"  batch {i + 1} loss: {last_loss}\")\n",
    "            tb_x = epoch_index * len(train_loader) + i + 1\n",
    "            tb_writer.add_scalar(\"Loss/train\", last_loss, tb_x)\n",
    "            running_loss = 0.0\n",
    "\n",
    "    return last_loss\n",
    "\n",
    "\n",
    "def run_training(\n",
    "    num_epochs: int,\n",
    "    model: GraphModel,\n",
    "    train_loader: DataLoader,\n",
    "    validation_loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
    "    timestamp: str,\n",
    "    device: torch.device,\n",
    "    verbose: bool = True,\n",
    ") -> str:\n",
    "    model_path = f\"models/{model.get_class_name()}_{timestamp}\"\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "    writer = SummaryWriter(f\"{model_path}/run\")\n",
    "    best_vloss = 1_000_000_000_000_000.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        model.train(True)\n",
    "        avg_loss = train_one_epoch(\n",
    "            epoch, model, train_loader, optimizer, loss_fn, writer, device\n",
    "        )\n",
    "\n",
    "        # We don't need gradients on to do reporting\n",
    "        model.train(False)\n",
    "\n",
    "        running_vloss = 0.0\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vdata.to(device)\n",
    "            vinputs, vadjacency_matrix, vlabels = (\n",
    "                vdata.x.float(),\n",
    "                vdata.edge_index,\n",
    "                vdata.y.float(),\n",
    "            )\n",
    "            voutputs = model(vinputs, vadjacency_matrix)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        if verbose:\n",
    "            print(f\"LOSS train {avg_loss} valid {avg_vloss}\")\n",
    "\n",
    "        # Log the running loss averaged per batch\n",
    "        # for both training and validation\n",
    "        writer.add_scalars(\n",
    "            \"Training vs. Validation Loss\",\n",
    "            {\"Training\": avg_loss, \"Validation\": avg_vloss},\n",
    "            epoch + 1,\n",
    "        )\n",
    "        writer.flush()\n",
    "\n",
    "        # Track best performance, and save the model's state\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            torch.save(model.state_dict(), f\"{model_path}/state_dict_epoch{epoch}.pt\")\n",
    "    return model_path\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model: GraphModel,\n",
    "    dataloader: DataLoader,\n",
    "    metric: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    "    verbose: bool = False,\n",
    ") -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "\n",
    "        def _eval_batch(batch, model):\n",
    "            batch_inputs, batch_adjacency_matrix, batch_labels = (\n",
    "                batch.x.float(),\n",
    "                batch.edge_index,\n",
    "                batch.y.float(),\n",
    "            )\n",
    "            return model(batch_inputs, batch_adjacency_matrix), batch_labels\n",
    "\n",
    "        model.eval()\n",
    "        model.train(False)\n",
    "        model.to(device)\n",
    "        y_preds = torch.tensor([]).to(device)\n",
    "        y_true = torch.tensor([]).to(device)\n",
    "        for batch in tqdm(dataloader, disable=not (verbose)):\n",
    "            batch.to(device)\n",
    "            batch_y_preds, batch_y_true = _eval_batch(batch, model)\n",
    "            y_preds = torch.cat((y_preds, batch_y_preds))\n",
    "            y_true = torch.cat((y_true, batch_y_true))\n",
    "        y_preds = torch.squeeze(y_preds)\n",
    "    return metric(y_preds.to(device), y_true.to(device))\n",
    "\n",
    "\n",
    "def evaluate_best_model(\n",
    "    model_state_dir: str,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    test_loader: DataLoader,\n",
    "    metric: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
    "    device: torch.device,\n",
    "    verbose: bool = True,\n",
    ") -> dict[str, torch.Tensor]:\n",
    "    def find_latest_state_dict(dir: str) -> str:\n",
    "        latest_state_dict_path = sorted(\n",
    "            [\n",
    "                item\n",
    "                for item in os.listdir(dir)\n",
    "                if len(item.split(\"state_dict_epoch\")) == 2\n",
    "            ]\n",
    "        )[-1]\n",
    "        return os.path.join(dir, latest_state_dict_path)\n",
    "\n",
    "    best_state_dict = torch.load(\n",
    "        find_latest_state_dict(model_state_dir), map_location=device\n",
    "    )\n",
    "    model.load_state_dict(best_state_dict)\n",
    "    evaluation = {\n",
    "        f\"Train {metric}\": evaluate_model(\n",
    "            model=model,\n",
    "            dataloader=train_loader,\n",
    "            metric=metric,\n",
    "            device=device,\n",
    "            verbose=verbose,\n",
    "        ),\n",
    "        f\"Val {metric}\": evaluate_model(\n",
    "            model=model,\n",
    "            dataloader=val_loader,\n",
    "            metric=metric,\n",
    "            device=device,\n",
    "            verbose=verbose,\n",
    "        ),\n",
    "        f\"Test {metric}\": evaluate_model(\n",
    "            model=model,\n",
    "            dataloader=test_loader,\n",
    "            metric=metric,\n",
    "            device=device,\n",
    "            verbose=verbose,\n",
    "        ),\n",
    "    }\n",
    "    return evaluation\n",
    "\n",
    "\n",
    "def denormalize_evaluation(\n",
    "    evaluation: dict[str, torch.Tensor],\n",
    "    storage_path: str,\n",
    "    split_feature_storage_file: str,\n",
    ") -> dict[str, torch.Tensor]:\n",
    "    with open(\n",
    "        f\"{storage_path}/raw/{split_feature_storage_file}\",\n",
    "        \"rb\",\n",
    "    ) as file:\n",
    "        fs: FeatureStorage = pickle.load(file)\n",
    "    evaluation_dict = copy(evaluation)\n",
    "    # Get normalized scores (MAE/L1 loss), assuming train comes 1st, val 2nd, and test 3rd in the dict\n",
    "    keys = list(evaluation_dict.keys())\n",
    "    train_key = keys[0]\n",
    "    val_key = keys[1]\n",
    "    test_key = keys[2]\n",
    "    normed_train_score = evaluation_dict[train_key]\n",
    "    normed_val_score = evaluation_dict[val_key]\n",
    "    normed_test_score = evaluation_dict[test_key]\n",
    "\n",
    "    evaluation_dict[train_key] = fs.scaler.inverse_transform([normed_train_score] * 25)[\n",
    "        -2\n",
    "    ]\n",
    "    evaluation_dict[val_key] = fs.scaler.inverse_transform([normed_val_score] * 25)[-2]\n",
    "    evaluation_dict[test_key] = fs.scaler.inverse_transform([normed_test_score] * 25)[\n",
    "        -2\n",
    "    ]\n",
    "\n",
    "    return evaluation_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get experiment configuration variables\n",
    "    (\n",
    "        FEATURE_SET,\n",
    "        LOG_PARAMETERS,\n",
    "        ORIGINAL_LOG_FILE,\n",
    "        STORAGE_PATH,\n",
    "        SPLIT_FEATURE_STORAGE_FILE,\n",
    "        RAW_FEATURE_STORAGE_FILE,\n",
    "        TARGET_LABEL,\n",
    "        LOCAL_ROOT_DIR,\n",
    "        LOAD_PREVIOUS_RUN,\n",
    "        RANDOM_SEED,\n",
    "        SUBGRAPH_SIZE,\n",
    "        EPOCHS,\n",
    "        BATCH_SIZE,\n",
    "        generator,\n",
    "        seed_worker,\n",
    "        device,\n",
    "        verbose,\n",
    "    ) = get_experiment_configuration(adams_config, verbose=True)\n",
    "\n",
    "    # Get data and dataloaders\n",
    "    prepare_feature_storage(\n",
    "        ORIGINAL_LOG_FILE,\n",
    "        LOG_PARAMETERS,\n",
    "        FEATURE_SET,\n",
    "        STORAGE_PATH,\n",
    "        SPLIT_FEATURE_STORAGE_FILE,\n",
    "        RAW_FEATURE_STORAGE_FILE,\n",
    "        RANDOM_SEED,\n",
    "    )\n",
    "    ds_train, ds_val, ds_test = load_datasets(\n",
    "        STORAGE_PATH,\n",
    "        SPLIT_FEATURE_STORAGE_FILE,\n",
    "        TARGET_LABEL,\n",
    "        train=True,\n",
    "        val=True,\n",
    "        test=True,\n",
    "    )\n",
    "    train_loader, val_loader, test_loader = prepare_dataloaders(\n",
    "        batch_size=BATCH_SIZE,\n",
    "        ds_train=ds_train,\n",
    "        ds_val=ds_val,\n",
    "        ds_test=ds_test,\n",
    "        seed_worker=seed_worker,\n",
    "        generator=generator,\n",
    "    )\n",
    "\n",
    "    # Set model configuration\n",
    "    model = configure_model(\n",
    "        ds_train.num_node_features, ds_train.num_node_features, SUBGRAPH_SIZE, device\n",
    "    )\n",
    "\n",
    "    # Print summary of data and model\n",
    "    if verbose:\n",
    "        print_dataset_summaries(ds_train, ds_val, ds_test)\n",
    "        print(model)\n",
    "        print(f\"Number of parameters: {count_parameters(model)}\")\n",
    "\n",
    "    # Load best model of completed training (if directory given)\n",
    "    model_path = LOAD_PREVIOUS_RUN[\"state_dict_dir\"]\n",
    "\n",
    "    # Run training\n",
    "    if LOAD_PREVIOUS_RUN[\"run_train_loop_again\"]:\n",
    "        print(\"Training started, progress available in Tensorboard\")\n",
    "        model_path = run_training(\n",
    "            num_epochs=EPOCHS,\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            validation_loader=val_loader,\n",
    "            optimizer=torch.optim.Adam(\n",
    "                model.parameters(),\n",
    "                lr=0.01,\n",
    "            ),\n",
    "            loss_fn=torch.nn.L1Loss(),\n",
    "            timestamp=datetime.now().strftime(\"%Y%m%d_%Hh%Mm\"),\n",
    "            device=device,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "    # Get MAE results\n",
    "    normalized_evaluation_dict = evaluate_best_model(\n",
    "        model_state_dir=model_path,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        metric=torch.nn.L1Loss(),\n",
    "        device=torch.device(\"cpu\"),\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    denormalized_evaluation_dict = denormalize_evaluation(\n",
    "        normalized_evaluation_dict,\n",
    "        STORAGE_PATH,\n",
    "        SPLIT_FEATURE_STORAGE_FILE,\n",
    "    )\n",
    "\n",
    "    # Print experiment results\n",
    "    print(\"Normalized:  \", normalized_evaluation_dict)\n",
    "    print(\"Denormalized:\", denormalized_evaluation_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
