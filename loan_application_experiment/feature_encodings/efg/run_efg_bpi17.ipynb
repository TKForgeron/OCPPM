{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python native\n",
    "import os\n",
    "os.chdir(\"/home/tim/Development/OCPPM/\")\n",
    "\n",
    "import pickle\n",
    "import pprint\n",
    "import random\n",
    "import json\n",
    "from copy import copy\n",
    "from datetime import datetime\n",
    "from statistics import median as median\n",
    "from sys import platform\n",
    "from typing import Any, Callable\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import ocpa.algo.predictive_monitoring.factory as feature_factory\n",
    "\n",
    "# PyG\n",
    "import torch\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "import torch.utils.tensorboard\n",
    "\n",
    "# Object centric process mining\n",
    "from ocpa.algo.predictive_monitoring.obj import Feature_Storage as FeatureStorage\n",
    "\n",
    "# # Simple machine learning models, procedure tools, and evaluation metrics\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "from tqdm import tqdm\n",
    "from torch import tensor\n",
    "\n",
    "# Custom imports\n",
    "# from loan_application_experiment.feature_encodings.efg.efg import EFG\n",
    "from loan_application_experiment.feature_encodings.efg.efg_sg import EFG_SG\n",
    "from loan_application_experiment.feature_encodings.efg.efg import EFG\n",
    "from custom_utilities import evaluation_helpers\n",
    "\n",
    "# from importing_ocel import build_feature_storage, load_ocel, pickle_feature_storage\n",
    "from loan_application_experiment.models.geometric_models import (\n",
    "    AGNN_EFG,\n",
    "    AdamsGCN,\n",
    "    GraphModel,\n",
    "    HigherOrderGNN_EFG,\n",
    "    SimpleGNN_EFG\n",
    ")\n",
    "import torch_geometric.nn as pygnn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as O\n",
    "import torch.nn as nn\n",
    "\n",
    "# Setup\n",
    "bpi17_config = {\n",
    "    \"STORAGE_PATH\": \"data/BPI17/feature_encodings/EFG/efg\",\n",
    "    \"SPLIT_FEATURE_STORAGE_FILE\": \"BPI_split_[C2_P2_P3_P5_O3_Action_EventOrigin_OrgResource].fs\",\n",
    "    \"TARGET_LABEL\": (feature_factory.EVENT_REMAINING_TIME, ()),\n",
    "    \"SUBGRAPH_SIZE\": 4,\n",
    "    \"BATCH_SIZE\": 64,\n",
    "    \"RANDOM_SEED\": 42,\n",
    "    \"EPOCHS\": 30,\n",
    "    \"early_stopping\": 5,\n",
    "    \"optimizer_settings\": {\n",
    "        \"lr\": 0.001,\n",
    "        \"betas\": (0.9, 0.999),\n",
    "        \"eps\": 1e-08,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": False,\n",
    "    },\n",
    "    \"loss_fn\": torch.nn.L1Loss(),\n",
    "    \"verbose\": True,\n",
    "    \"skip_cache\": False,\n",
    "}\n",
    "\n",
    "# ADAPTATIONS\n",
    "# bpi17_config[\"optimizer_settings\"] = {\n",
    "#     \"lr\": 5e-4,\n",
    "#     \"betas\": (0.9, 0.999),\n",
    "#     \"eps\": 1e-08,\n",
    "#     \"weight_decay\": 0,\n",
    "#     \"amsgrad\": False,\n",
    "# }\n",
    "# bpi17_config[\"loss_fn\"] = torch.nn.L1Loss()\n",
    "# bpi17_config[\"BATCH_SIZE\"] = 64\n",
    "# bpi17_config[\"EPOCHS\"] = 30\n",
    "# bpi17_config[\"early_stopping\"] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(\n",
    "    storage_path: str,\n",
    "    split_feature_storage_file: str,\n",
    "    target_label: tuple[str, tuple],\n",
    "    subgraph_size: int,\n",
    "    transform=None,\n",
    "    train: bool = True,\n",
    "    val: bool = True,\n",
    "    test: bool = True,\n",
    "    skip_cache: bool = False,\n",
    ") -> list[EFG_SG or EFG]:\n",
    "    datasets = []\n",
    "    if train:\n",
    "        ds_train = EFG_SG(\n",
    "            train=True,\n",
    "            root=storage_path,\n",
    "            filename=split_feature_storage_file,\n",
    "            label_key=target_label,\n",
    "            size_subgraph_samples=subgraph_size,\n",
    "            transform=transform,\n",
    "            verbosity=51,\n",
    "            skip_cache=skip_cache,\n",
    "        )\n",
    "        datasets.append(ds_train)\n",
    "    if val:\n",
    "        ds_val = EFG_SG(\n",
    "            validation=True,\n",
    "            root=storage_path,\n",
    "            filename=split_feature_storage_file,\n",
    "            label_key=target_label,\n",
    "            size_subgraph_samples=subgraph_size,\n",
    "            transform=transform,\n",
    "            verbosity=51,\n",
    "            skip_cache=skip_cache,\n",
    "        )\n",
    "        datasets.append(ds_val)\n",
    "    if test:\n",
    "        ds_test = EFG_SG(\n",
    "            test=True,\n",
    "            root=storage_path,\n",
    "            filename=split_feature_storage_file,\n",
    "            label_key=target_label,\n",
    "            size_subgraph_samples=subgraph_size,\n",
    "            transform=transform,\n",
    "            verbosity=51,\n",
    "            skip_cache=skip_cache,\n",
    "        )\n",
    "        datasets.append(ds_test)\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def print_dataset_summaries(\n",
    "    ds_train: EFG_SG or EFG,\n",
    "    ds_val: EFG_SG or EFG,\n",
    "    ds_test: EFG_SG or EFG,\n",
    ") -> None:\n",
    "    print(\"Train set\")\n",
    "    print(ds_train.get_summary(), \"\\n\")\n",
    "    print(\"Validation set\")\n",
    "    print(ds_val.get_summary(), \"\\n\")\n",
    "    print(\"Test set\")\n",
    "    print(ds_test.get_summary(), \"\\n\")\n",
    "\n",
    "\n",
    "def configure_adams_model(\n",
    "    num_node_features: int,\n",
    "    num_hidden_features: int,\n",
    "    size_subgraph_samples: int,\n",
    "    device: torch.device,\n",
    ") -> GraphModel:\n",
    "    # Initialize model\n",
    "    model = AdamsGCN(\n",
    "        num_node_features=num_node_features,\n",
    "        hyperparams={\n",
    "            \"num_hidden_features\": num_hidden_features,\n",
    "            \"size_subgraph_samples\": size_subgraph_samples,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    model = model.to(device)\n",
    "    # data = ds_train.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "def count_parameters(model: GraphModel) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def prepare_dataloaders(\n",
    "    batch_size: int,\n",
    "    ds_train: EFG_SG or EFG = None,\n",
    "    ds_val: EFG_SG  or EFG= None,\n",
    "    ds_test: EFG_SG  or EFG= None,\n",
    "    shuffle: bool = True,\n",
    "    pin_memory: bool = True,\n",
    "    num_workers: int = 4,\n",
    "    seed_worker: Callable[[int], None] = None,\n",
    "    generator: torch.Generator = None,\n",
    ") -> list[DataLoader]:\n",
    "    dataloaders = []\n",
    "    if ds_train:\n",
    "        train_loader = DataLoader(\n",
    "            ds_train,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            pin_memory=pin_memory,\n",
    "            num_workers=num_workers,\n",
    "            worker_init_fn=seed_worker,\n",
    "            generator=generator,\n",
    "        )\n",
    "        dataloaders.append(train_loader)\n",
    "    if ds_val:\n",
    "        val_loader = DataLoader(\n",
    "            ds_val,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            pin_memory=pin_memory,\n",
    "            num_workers=num_workers,\n",
    "            worker_init_fn=seed_worker,\n",
    "            generator=generator,\n",
    "        )\n",
    "        dataloaders.append(val_loader)\n",
    "    if ds_test:\n",
    "        test_loader = DataLoader(\n",
    "            ds_test,\n",
    "            batch_size=128,\n",
    "            shuffle=shuffle,\n",
    "            pin_memory=pin_memory,\n",
    "            num_workers=num_workers,\n",
    "            worker_init_fn=seed_worker,\n",
    "            generator=generator,\n",
    "        )\n",
    "        dataloaders.append(test_loader)\n",
    "    return dataloaders\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    epoch_index: int,\n",
    "    model: GraphModel,\n",
    "    train_loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
    "    tb_writer: SummaryWriter,\n",
    "    device: torch.device,\n",
    "    verbose: bool = True,\n",
    ") -> float:\n",
    "    if verbose:\n",
    "        print(f\"EPOCH {epoch_index + 1}:\")\n",
    "\n",
    "    # Enumerate over the data\n",
    "    running_loss = 0.0\n",
    "    last_loss = 0\n",
    "    for i, batch in enumerate(tqdm(train_loader, miniters=25)):\n",
    "        # Use GPU\n",
    "        batch.to(device)\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, adjacency_matrix, labels = (\n",
    "            batch.x.float(),  # k times the batch_size, where k is the subgraph size\n",
    "            batch.edge_index,\n",
    "            batch.y.float(),\n",
    "        )\n",
    "        # Reset gradients (set_to_none is faster than to zero)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        # Passing the node features and the connection info\n",
    "        outputs = model(inputs, edge_index=adjacency_matrix, batch=batch.batch)\n",
    "        # Compute loss and gradients\n",
    "        loss = loss_fn(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        # Adjust learnable weights\n",
    "        optimizer.step()\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000  # loss per batch\n",
    "            if verbose:\n",
    "                print(f\"  batch {i + 1} loss: {last_loss}\")\n",
    "            tb_x = epoch_index * len(train_loader) + i + 1\n",
    "            tb_writer.add_scalar(\"Loss/train\", last_loss, tb_x)\n",
    "            running_loss = 0.0\n",
    "\n",
    "    return last_loss\n",
    "\n",
    "\n",
    "def run_training(\n",
    "    num_epochs: int,\n",
    "    model: GraphModel,\n",
    "    train_loader: DataLoader,\n",
    "    validation_loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
    "    early_stopping_criterion: int,\n",
    "    timestamp: str,\n",
    "    device: torch.device,\n",
    "    verbose: bool = True,\n",
    ") -> str:\n",
    "    model_path = f\"models/runs/{model.get_class_name()}_{timestamp}\"\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "    writer = SummaryWriter(f\"{model_path}/run\")\n",
    "    best_vloss = 1_000_000_000_000_000.0\n",
    "    epochs_without_improvement = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        model.train(True)\n",
    "        avg_loss = train_one_epoch(\n",
    "            epoch, model, train_loader, optimizer, loss_fn, writer, device, verbose\n",
    "        )\n",
    "\n",
    "        # We don't need gradients on to do reporting\n",
    "        model.train(False)\n",
    "\n",
    "        running_vloss = 0.0\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vdata.to(device)\n",
    "            vinputs, vadjacency_matrix, vlabels = (\n",
    "                vdata.x.float(),\n",
    "                vdata.edge_index,\n",
    "                vdata.y.float(),\n",
    "            )\n",
    "            voutputs = model(vinputs, edge_index=vadjacency_matrix, batch=vdata.batch)\n",
    "            vloss = loss_fn(voutputs.squeeze(), vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        if verbose:\n",
    "            print(f\"LOSS train {avg_loss} valid {avg_vloss}\")\n",
    "\n",
    "        # Log the running loss averaged per batch\n",
    "        # for both training and validation\n",
    "        writer.add_scalars(\n",
    "            \"Training vs. Validation Loss\",\n",
    "            {\"Training\": avg_loss, \"Validation\": avg_vloss},\n",
    "            epoch + 1,\n",
    "        )\n",
    "        writer.flush()\n",
    "\n",
    "        # Track best performance, and save the model's state\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            epochs_without_improvement = 0\n",
    "            torch.save(model.state_dict(), f\"{model_path}/state_dict_epoch{epoch}.pt\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= early_stopping_criterion:\n",
    "            print(f\"Early stopping after {epoch+1} epochs.\")\n",
    "            break\n",
    "    return model_path\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model: GraphModel,\n",
    "    dataloader: DataLoader,\n",
    "    evaluation_reporter: Callable[[torch.Tensor, torch.Tensor, bool, bool], dict[str,dict[str,Any]]],\n",
    "    regression:bool,\n",
    "    classification:bool,\n",
    "    verbose: bool = False,\n",
    ") -> dict[str,dict[str,Any]]:\n",
    "    device=torch.device(\"cpu\")\n",
    "    with torch.no_grad():\n",
    "\n",
    "        def _eval_batch(batch, model):\n",
    "            batch_inputs, batch_adjacency_matrix, batch_labels = (\n",
    "                batch.x.float(),\n",
    "                batch.edge_index,\n",
    "                batch.y.float(),\n",
    "            )\n",
    "            return (\n",
    "                model(\n",
    "                    batch_inputs, edge_index=batch_adjacency_matrix, batch=batch.batch\n",
    "                ),\n",
    "                batch_labels,\n",
    "            )\n",
    "\n",
    "        model.eval()\n",
    "        model.train(False)\n",
    "        model.to(device)\n",
    "        y_preds = torch.tensor([]).to(device)\n",
    "        y_true = torch.tensor([]).to(device)\n",
    "        for batch in tqdm(dataloader, disable=not (verbose)):\n",
    "            batch.to(device)\n",
    "            batch_y_preds, batch_y_true = _eval_batch(batch, model)\n",
    "            # append current batch prediction\n",
    "            y_preds = torch.cat((y_preds, batch_y_preds))\n",
    "            y_true = torch.cat((y_true, batch_y_true))\n",
    "        y_preds = torch.squeeze(y_preds)\n",
    "    return evaluation_reporter(y_preds.to(device), y_true.to(device),regression,classification)\n",
    "\n",
    "\n",
    "def get_best_model_evaluation(\n",
    "    model_state_dir: str,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    test_loader: DataLoader,\n",
    "    model: GraphModel,\n",
    "    evaluation_reporter: Callable[[torch.Tensor, torch.Tensor, bool, bool], dict[str,dict[str,Any]]],\n",
    "    regression:bool,\n",
    "    classification:bool,\n",
    "    verbose: bool = True,\n",
    ") -> dict[str,dict[str,Any]]:\n",
    "    def find_latest_state_dict(dir: str) -> str:\n",
    "        latest_state_dict_path = sorted(\n",
    "            [\n",
    "                item\n",
    "                for item in os.listdir(dir)\n",
    "                if len(item.split(\"state_dict_epoch\")) == 2\n",
    "            ]\n",
    "        )[-1]\n",
    "        return os.path.join(dir, latest_state_dict_path)\n",
    "\n",
    "    best_state_dict = torch.load(\n",
    "        find_latest_state_dict(model_state_dir)#, map_location=device\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(best_state_dict)\n",
    "    model.eval()\n",
    "    evaluation = {\n",
    "        f\"Train\": evaluate_model(\n",
    "            model=model,\n",
    "            dataloader=train_loader,\n",
    "            evaluation_reporter=evaluation_reporter,\n",
    "            regression=regression,\n",
    "            classification=classification,\n",
    "            verbose=verbose,\n",
    "        ),\n",
    "        f\"Validation\": evaluate_model(\n",
    "            model=model,\n",
    "            dataloader=val_loader,\n",
    "            evaluation_reporter=evaluation_reporter,\n",
    "            regression=regression,\n",
    "            classification=classification,\n",
    "            verbose=verbose,\n",
    "        ),\n",
    "        f\"Test\": evaluate_model(\n",
    "            model=model,\n",
    "            dataloader=test_loader,\n",
    "            evaluation_reporter=evaluation_reporter,\n",
    "            regression=regression,\n",
    "            classification=classification,\n",
    "            verbose=verbose,\n",
    "        ),\n",
    "    }\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "def seed_worker(worker_id: int) -> None:\n",
    "    # worker_seed = torch.initial_seed() % RANDOM_SEED\n",
    "    worker_seed = bpi17_config[\"RANDOM_SEED\"]\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "\n",
    "generator = torch.Generator().manual_seed(bpi17_config[\"RANDOM_SEED\"])\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Get data and dataloaders\n",
    "ds_train, ds_val, ds_test = load_datasets(\n",
    "    bpi17_config[\"STORAGE_PATH\"],\n",
    "    bpi17_config[\"SPLIT_FEATURE_STORAGE_FILE\"],\n",
    "    bpi17_config[\"TARGET_LABEL\"],\n",
    "    bpi17_config[\"SUBGRAPH_SIZE\"],\n",
    "    train=True,\n",
    "    val=True,\n",
    "    test=True,\n",
    "    skip_cache=bpi17_config[\"skip_cache\"],\n",
    ")\n",
    "train_loader, val_loader, test_loader = prepare_dataloaders(\n",
    "    batch_size=bpi17_config[\"BATCH_SIZE\"],\n",
    "    ds_train=ds_train,\n",
    "    ds_val=ds_val,\n",
    "    ds_test=ds_test,\n",
    "    seed_worker=seed_worker,\n",
    "    generator=generator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HigherOrderGNN_EFG(\n",
      "  (conv1): GraphConv(-1, 48)\n",
      "  (conv2): GraphConv(-1, 48)\n",
      "  (act1): PReLU(num_parameters=1)\n",
      "  (act2): PReLU(num_parameters=1)\n",
      "  (lin_out): Linear(-1, 1, bias=True)\n",
      ")\n",
      "Number of parameters: 7347\n"
     ]
    }
   ],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HigherOrderGNN_EFG(48, 1)\n",
    "# pretrained_state_dict = torch.load(\"models/runs/GraphConvNet_20230718_13h59m/state_dict_epoch6.pt\")\n",
    "# model.load_state_dict(pretrained_state_dict)\n",
    "model.to(device)\n",
    "\n",
    "# Print summary of data and model\n",
    "if bpi17_config[\"verbose\"]:\n",
    "    print(model)\n",
    "    with torch.no_grad():  # Initialize lazy modules, s.t. we can count its parameters.\n",
    "        batch = next(iter(train_loader))\n",
    "        batch.to(device)\n",
    "        out = model(batch.x.float(), batch.edge_index, batch.batch)\n",
    "        print(f\"Number of parameters: {count_parameters(model)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "print(\"Training started, progress available in Tensorboard\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_path = run_training(\n",
    "    num_epochs=bpi17_config[\"EPOCHS\"],\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    "    optimizer=O.Adam(\n",
    "        model.parameters(), **bpi17_config[\"optimizer_settings\"]\n",
    "    ),\n",
    "    loss_fn=bpi17_config[\"loss_fn\"],\n",
    "    early_stopping_criterion=bpi17_config[\"early_stopping\"],\n",
    "    timestamp=datetime.now().strftime(\"%Y%m%d_%Hh%Mm\"),\n",
    "    device=device,\n",
    "    verbose=True,\n",
    ")\n",
    "# Write experiment settings as JSON into model path\n",
    "with open(os.path.join(model_path, \"experiment_settings.json\"), \"w\") as file_path:\n",
    "    bpi17_config[\"loss_fn\"] = str(bpi17_config[\"loss_fn\"])\n",
    "    json.dump(bpi17_config, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test': {'report': {'MAE': 0.4040075,\n",
      "                     'MAPE': 4.1964836,\n",
      "                     'MSE': 0.46823576,\n",
      "                     'R^2': -0.02102018627294422}},\n",
      " 'Train': {'report': {'MAE': 0.40583354,\n",
      "                      'MAPE': 5.2838283,\n",
      "                      'MSE': 0.4805708,\n",
      "                      'R^2': -0.02500405303885933}},\n",
      " 'Validation': {'report': {'MAE': 0.4187696,\n",
      "                           'MAPE': 4.9057894,\n",
      "                           'MSE': 0.49645367,\n",
      "                           'R^2': -0.07418939895318877}}}\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "state_dict_path = \"models/runs/AGNN_20230714_12h19m\"  # 0.59 test mae\n",
    "state_dict_path = \"models/runs/AGNN_20230714_14h26m\"  # 0.54 test mae\n",
    "state_dict_path = \"models/runs/AGNN_20230717_15h16m\"  # 0.48 test mae ()\n",
    "state_dict_path = \"models/runs/AGNN_20230717_16h37m\"  # 0.47 test mae\n",
    "state_dict_path = \"models/runs/AGNN_20230717_15h51m\"  # 0.4557 test mae (ChebConv)\n",
    "state_dict_path = \"models/runs/AGNN_20230717_16h58m\"  # 0.4546 test mae\n",
    "state_dict_path = \"models/runs/AGNN_20230717_23h22m\"  # 0.4534 test mae\n",
    "state_dict_path = \"models/runs/SimpleGNN_20230718_09h30m\"  # 0.4382 test mae | 6k params\n",
    "state_dict_path = \"models/runs/TransformerGNN_20230718_09h46m\"  # 0.4290 test mae | 24k params\n",
    "state_dict_path = \"models/runs/GraphConvArch_20230718_10h08m\"  # 0.4248 test mae | 12k params\n",
    "state_dict_path = \"models/runs/GraphConvNet_20230718_11h35m\"  # 0.4149 test mae | 7k params\n",
    "state_dict_path = \"models/runs/GraphConvNet_20230718_11h54m\"  # 0.4113 test mae | 7k params\n",
    "state_dict_path = \"models/runs/GraphConvNet_20230718_13h59m\"  # 0.4040 test mae | 7k params | fine-tuning pretrained 'GraphConvNet_20230718_11h54m'  // best so far!\n",
    "\n",
    "# Get model evaluation report\n",
    "evaluation_report = get_best_model_evaluation(\n",
    "    model_state_dir=state_dict_path,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    model=model,\n",
    "    evaluation_reporter=evaluation_helpers.get_evaluation,\n",
    "    regression=True,\n",
    "    classification=False,\n",
    "    verbose=bpi17_config[\"verbose\"],\n",
    ")\n",
    "\n",
    "# Print evaluation report\n",
    "pprint.pprint(evaluation_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
