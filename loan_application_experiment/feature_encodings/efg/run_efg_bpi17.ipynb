{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/Development/OCPPM/.env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.13.1+cu117\n",
      "Cuda available: True\n",
      "Torch geometric version: 2.3.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DEPENDENCIES\n",
    "# Python native\n",
    "import os\n",
    "\n",
    "os.chdir(\"/home/tim/Development/OCPPM/\")\n",
    "\n",
    "import pickle\n",
    "import pprint\n",
    "import random\n",
    "import functools\n",
    "import json\n",
    "from copy import copy\n",
    "from datetime import datetime\n",
    "from statistics import median as median\n",
    "from sys import platform\n",
    "from typing import Any, Callable\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import ocpa.algo.predictive_monitoring.factory as feature_factory\n",
    "\n",
    "# PyG\n",
    "import torch\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "import torch.utils.tensorboard\n",
    "\n",
    "# Object centric process mining\n",
    "from ocpa.algo.predictive_monitoring.obj import Feature_Storage as FeatureStorage\n",
    "\n",
    "# # Simple machine learning models, procedure tools, and evaluation metrics\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "from tqdm import tqdm\n",
    "from torch import tensor\n",
    "\n",
    "# Custom imports\n",
    "# from loan_application_experiment.feature_encodings.efg.efg import EFG\n",
    "from loan_application_experiment.feature_encodings.efg.efg_sg import EFG_SG\n",
    "from utilities import torch_utils\n",
    "from utilities import data_utils\n",
    "from utilities import training_utils\n",
    "from utilities import evaluation_utils\n",
    "\n",
    "# from importing_ocel import build_feature_storage, load_ocel, pickle_feature_storage\n",
    "from loan_application_experiment.models.geometric_models import (\n",
    "    AGNN_EFG,\n",
    "    AdamsGCN,\n",
    "    GraphModel,\n",
    "    HigherOrderGNN_EFG,\n",
    "    SimpleGNN_EFG,\n",
    ")\n",
    "import torch_geometric.nn as pygnn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as O\n",
    "import torch.nn as nn\n",
    "\n",
    "# Print system info\n",
    "torch_utils.print_system_info()\n",
    "\n",
    "# Setup\n",
    "bpi17_config = {\n",
    "    \"STORAGE_PATH\": \"data/BPI17/feature_encodings/EFG/efg\",\n",
    "    \"SPLIT_FEATURE_STORAGE_FILE\": \"BPI_split_[C2_P2_P3_P5_O3_Action_EventOrigin_OrgResource].fs\",\n",
    "    \"TARGET_LABEL\": (feature_factory.EVENT_REMAINING_TIME, ()),\n",
    "    \"SUBGRAPH_SIZE\": 4,\n",
    "    \"BATCH_SIZE\": 64,\n",
    "    \"RANDOM_SEED\": 42,\n",
    "    \"EPOCHS\": 30,\n",
    "    \"early_stopping\": 5,\n",
    "    \"optimizer_settings\": {\n",
    "        \"lr\": 0.001,\n",
    "        \"betas\": (0.9, 0.999),\n",
    "        \"eps\": 1e-08,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": False,\n",
    "    },\n",
    "    \"loss_fn\": torch.nn.L1Loss(),\n",
    "    \"verbose\": True,\n",
    "    \"skip_cache\": False,\n",
    "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}\n",
    "\n",
    "# ADAPTATIONS\n",
    "# bpi17_config[\"optimizer_settings\"] = {\n",
    "#     \"lr\": 5e-4,\n",
    "#     \"betas\": (0.9, 0.999),\n",
    "#     \"eps\": 1e-08,\n",
    "#     \"weight_decay\": 0,\n",
    "#     \"amsgrad\": False,\n",
    "# }\n",
    "# bpi17_config[\"loss_fn\"] = torch.nn.L1Loss()\n",
    "# bpi17_config[\"BATCH_SIZE\"] = 64\n",
    "# bpi17_config[\"EPOCHS\"] = 30\n",
    "# bpi17_config[\"early_stopping\"] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data and dataloaders\n",
    "ds_train, ds_val, ds_test = data_utils.load_datasets(\n",
    "    bpi17_config[\"STORAGE_PATH\"],\n",
    "    bpi17_config[\"SPLIT_FEATURE_STORAGE_FILE\"],\n",
    "    bpi17_config[\"TARGET_LABEL\"],\n",
    "    bpi17_config[\"SUBGRAPH_SIZE\"],\n",
    "    train=True,\n",
    "    val=True,\n",
    "    test=True,\n",
    "    skip_cache=bpi17_config[\"skip_cache\"],\n",
    ")\n",
    "train_loader, val_loader, test_loader = data_utils.prepare_dataloaders(\n",
    "    batch_size=bpi17_config[\"BATCH_SIZE\"],\n",
    "    ds_train=ds_train,\n",
    "    ds_val=ds_val,\n",
    "    ds_test=ds_test,\n",
    "    seed_worker=functools.partial(\n",
    "        torch_utils.seed_worker, state=bpi17_config[\"RANDOM_SEED\"]\n",
    "    ),\n",
    "    generator=torch.Generator().manual_seed(bpi17_config[\"RANDOM_SEED\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HigherOrderGNN_EFG(\n",
      "  (conv1): GraphConv(-1, 48)\n",
      "  (conv2): GraphConv(-1, 48)\n",
      "  (act1): PReLU(num_parameters=1)\n",
      "  (act2): PReLU(num_parameters=1)\n",
      "  (lin_out): Linear(-1, 1, bias=True)\n",
      ")\n",
      "Number of parameters: 7347\n"
     ]
    }
   ],
   "source": [
    "model = HigherOrderGNN_EFG(48, 1)\n",
    "# pretrained_state_dict = torch.load(\"models/runs/GraphConvNet_20230718_13h59m/state_dict_epoch6.pt\")\n",
    "# model.load_state_dict(pretrained_state_dict)\n",
    "model.to(bpi17_config[\"device\"])\n",
    "\n",
    "# Print summary of data and model\n",
    "if bpi17_config[\"verbose\"]:\n",
    "    print(model)\n",
    "    with torch.no_grad():  # Initialize lazy modules, s.t. we can count its parameters.\n",
    "        batch = next(iter(train_loader))\n",
    "        batch.to(bpi17_config[\"device\"])\n",
    "        out = model(batch.x.float(), batch.edge_index, batch.batch)\n",
    "        print(f\"Number of parameters: {torch_utils.count_parameters(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "print(\"Training started, progress available in Tensorboard\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%Hh%Mm\")\n",
    "model_path_base = f\"models/BPI17/efg/{str(model).split('(')[0]}_{timestamp}\"\n",
    "\n",
    "best_state_dict_path = training_utils.run_training(\n",
    "    num_epochs=bpi17_config[\"EPOCHS\"],\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    "    optimizer=O.Adam(model.parameters(), **bpi17_config[\"optimizer_settings\"]),\n",
    "    loss_fn=bpi17_config[\"loss_fn\"],\n",
    "    early_stopping_criterion=bpi17_config[\"early_stopping\"],\n",
    "    model_path_base=model_path_base,\n",
    "    device=bpi17_config[\"device\"],\n",
    "    verbose=True,\n",
    ")\n",
    "# Write experiment settings as JSON into model path (of the model we've just trained)\n",
    "with open(os.path.join(model_path_base, \"experiment_settings.json\"), \"w\") as file_path:\n",
    "    json.dump(evaluation_utils.get_json_serializable_dict(bpi17_config), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2626/2626 [01:16<00:00, 34.35it/s]\n",
      "100%|██████████| 657/657 [00:18<00:00, 36.15it/s]\n",
      "100%|██████████| 699/699 [00:28<00:00, 24.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test': {'report': {'MAE': 0.4087477,\n",
      "                     'MAPE': 2.7260685,\n",
      "                     'MSE': 0.4682943,\n",
      "                     'R^2': -0.015702805917782614}},\n",
      " 'Train': {'report': {'MAE': 0.4119272,\n",
      "                      'MAPE': 6.021801,\n",
      "                      'MSE': 0.48191255,\n",
      "                      'R^2': -0.0268219392311162}},\n",
      " 'Validation': {'report': {'MAE': 0.42308843,\n",
      "                           'MAPE': 4.256179,\n",
      "                           'MSE': 0.49531737,\n",
      "                           'R^2': -0.07168010063702734}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "state_dict_path = \"models/BPI17/efg/AGNN_20230714_12h19m\"  # 0.59 test mae\n",
    "state_dict_path = \"models/BPI17/efg/AGNN_20230714_14h26m\"  # 0.54 test mae\n",
    "state_dict_path = \"models/BPI17/efg/AGNN_20230717_15h16m\"  # 0.48 test mae ()\n",
    "state_dict_path = \"models/BPI17/efg/AGNN_20230717_16h37m\"  # 0.47 test mae\n",
    "state_dict_path = \"models/BPI17/efg/AGNN_20230717_15h51m\"  # 0.4557 test mae (ChebConv)\n",
    "state_dict_path = \"models/BPI17/efg/AGNN_20230717_16h58m\"  # 0.4546 test mae\n",
    "state_dict_path = \"models/BPI17/efg/AGNN_20230717_23h22m\"  # 0.4534 test mae\n",
    "state_dict_path = (\n",
    "    \"models/BPI17/efg/SimpleGNN_20230718_09h30m\"  # 0.4382 test mae | 6k params\n",
    ")\n",
    "state_dict_path = (\n",
    "    \"models/BPI17/efg/TransformerGNN_20230718_09h46m\"  # 0.4290 test mae | 24k params\n",
    ")\n",
    "state_dict_path = (\n",
    "    \"models/BPI17/efg/GraphConvArch_20230718_10h08m\"  # 0.4248 test mae | 12k params\n",
    ")\n",
    "state_dict_path = (\n",
    "    \"models/BPI17/efg/GraphConvNet_20230718_11h35m\"  # 0.4149 test mae | 7k params\n",
    ")\n",
    "state_dict_path = (\n",
    "    \"models/BPI17/efg/GraphConvNet_20230718_11h54m\"  # 0.4113 test mae | 7k params\n",
    ")\n",
    "state_dict_path = \"models/BPI17/efg/GraphConvNet_20230718_13h59m\"  # 0.4040 test mae | 7k params | fine-tuning pretrained 'GraphConvNet_20230718_11h54m'  // best so far!\n",
    "state_dict_path = (\n",
    "    \"models/BPI17/efg/HigherOrderGNN_EFG_20230720_13h11m\"  # 0.4087 test mae | 7k params\n",
    ")\n",
    "\n",
    "# Get model evaluation report\n",
    "evaluation_report = evaluation_utils.get_best_model_evaluation(\n",
    "    model_state_dict_path=best_state_dict_path,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    model=model,\n",
    "    evaluation_reporter=evaluation_utils.get_evaluation,\n",
    "    regression=True,\n",
    "    classification=False,\n",
    "    verbose=bpi17_config[\"verbose\"],\n",
    ")\n",
    "\n",
    "# Store model results as JSON into model path\n",
    "with open(os.path.join(model_path_base, \"evaluation_report.json\"), \"w\") as file_path:\n",
    "    json.dump(evaluation_utils.get_json_serializable_dict(evaluation_report), file_path)\n",
    "\n",
    "# Print evaluation report\n",
    "pprint.pprint(evaluation_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
