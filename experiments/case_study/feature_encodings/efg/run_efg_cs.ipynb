{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: Intel(R) Core(TM) i5-7500 CPU @ 3.40GHz (4x)\n",
      "Total CPU memory: 46.93GB\n",
      "Available CPU memory: 41.66GB\n",
      "GPU: NVIDIA GeForce GTX 960\n",
      "Total GPU memory: 4096.0MB\n",
      "Available GPU memory: 4029.0MB\n",
      "Platform: Linux-5.19.0-46-generic-x86_64-with-glibc2.35\n"
     ]
    }
   ],
   "source": [
    "# DEPENDENCIES\n",
    "# Python native\n",
    "import os\n",
    "\n",
    "os.chdir(\"/home/tim/Development/OCPPM/\")\n",
    "\n",
    "import pprint\n",
    "import functools\n",
    "import gc\n",
    "import json\n",
    "from copy import copy\n",
    "from datetime import datetime\n",
    "from statistics import median as median\n",
    "from typing import Any, Callable\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "\n",
    "# PyG\n",
    "import torch\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "import torch.utils.tensorboard\n",
    "\n",
    "# # Simple machine learning models, procedure tools, and evaluation metrics\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "import ocpa.algo.predictive_monitoring.factory as feature_factory\n",
    "\n",
    "# Custom imports\n",
    "from experiments.efg import EFG\n",
    "from experiments.efg_sg import EFG_SG\n",
    "from utilities import torch_utils\n",
    "from utilities import data_utils\n",
    "from utilities import training_utils\n",
    "from utilities import evaluation_utils\n",
    "\n",
    "from models.definitions.geometric_models import (\n",
    "    AGNN_EFG,\n",
    "    AdamsGCN,\n",
    "    GraphModel,\n",
    "    HigherOrderGNN_EFG,\n",
    "    SimpleGNN_EFG,\n",
    ")\n",
    "import torch_geometric.nn as pygnn\n",
    "import torch.optim as O\n",
    "import torch.nn as nn\n",
    "\n",
    "# Print system info\n",
    "torch_utils.print_system_info()\n",
    "\n",
    "# Setup\n",
    "cs_efg_config = {\n",
    "    \"model_output_path\": \"models/CS/efg\",\n",
    "    \"STORAGE_PATH\": \"data/CS/feature_encodings/EFG/efg\",\n",
    "    \"SPLIT_FEATURE_STORAGE_FILE\": \"CS_split_[C2_P2_P3_O3_eas].fs\",\n",
    "    \"regression_task\": False,\n",
    "    \"TARGET_LABEL\": \"event_ea4\",\n",
    "    \"graph_level_prediction\": True,\n",
    "    \"features_dtype\": torch.float32,\n",
    "    \"target_dtype\": torch.int64,\n",
    "    \"class_distribution\": {\n",
    "        0.0: 0.705315,\n",
    "        1.0: 0.015818,\n",
    "        2.0: 0.010882,\n",
    "        3.0: 0.016800,\n",
    "        4.0: 0.069764,\n",
    "        5.0: 0.081383,\n",
    "        6.0: 0.100038,\n",
    "    },\n",
    "    \"SUBGRAPH_SIZE\": 4,\n",
    "    \"BATCH_SIZE\": 64,\n",
    "    \"RANDOM_SEED\": 42,\n",
    "    \"EPOCHS\": 30,\n",
    "    \"early_stopping\": 5,\n",
    "    \"optimizer\": O.Adam,\n",
    "    \"optimizer_settings\": {\n",
    "        \"lr\": 0.001,\n",
    "        \"betas\": (0.9, 0.999),\n",
    "        \"eps\": 1e-08,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": False,\n",
    "    },\n",
    "    \"loss_fn\": torch.nn.CrossEntropyLoss(),\n",
    "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"verbose\": True,\n",
    "    \"skip_cache\": False,\n",
    "}\n",
    "\n",
    "# ADAPTATIONS\n",
    "# reset config for rem_time prediction:\n",
    "cs_efg_config[\"regression_task\"] = True\n",
    "if cs_efg_config[\"regression_task\"]:\n",
    "    cs_efg_config[\"target_dtype\"] = torch.float32\n",
    "    cs_efg_config[\"TARGET_LABEL\"] = (feature_factory.EVENT_REMAINING_TIME, ())\n",
    "    cs_efg_config[\"loss_fn\"] = torch.nn.L1Loss()\n",
    "    if \"class_distribution\" in cs_efg_config:\n",
    "        del cs_efg_config[\"class_distribution\"]\n",
    "\n",
    "# other adaptations\n",
    "cs_efg_config[\"BATCH_SIZE\"] = 64\n",
    "cs_efg_config[\"early_stopping\"] = 4\n",
    "cs_efg_config[\"optimizer\"] = O.NAdam\n",
    "cs_efg_config[\"optimizer_settings\"] = {\n",
    "    \"lr\": 0.001,\n",
    "}\n",
    "# cs_efg_config[\"skip_cache\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No EventSubGraphDataset found with this configuration in 'data/CS/feature_encodings/EFG/efg/processed'. Proceeding to processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "61965it [05:43, 154.41it/s]"
     ]
    }
   ],
   "source": [
    "# Get data and dataloaders\n",
    "(ds_train, ds_val, ds_test) = data_utils.load_datasets(\n",
    "    dataset_class=EFG_SG,\n",
    "    storage_path=cs_efg_config[\"STORAGE_PATH\"],\n",
    "    split_feature_storage_file=cs_efg_config[\"SPLIT_FEATURE_STORAGE_FILE\"],\n",
    "    target_label=cs_efg_config[\"TARGET_LABEL\"],\n",
    "    graph_level_target=cs_efg_config[\"graph_level_prediction\"],\n",
    "    features_dtype=cs_efg_config[\"features_dtype\"],\n",
    "    target_dtype=cs_efg_config[\"target_dtype\"],\n",
    "    subgraph_size=cs_efg_config[\"SUBGRAPH_SIZE\"],\n",
    "    train=True,\n",
    "    val=True,\n",
    "    test=True,\n",
    "    skip_cache=cs_efg_config[\"skip_cache\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = data_utils.prepare_dataloaders(\n",
    "    batch_size=cs_efg_config[\"BATCH_SIZE\"],\n",
    "    ds_train=ds_train,\n",
    "    ds_val=ds_val,\n",
    "    ds_test=ds_test,\n",
    "    # num_workers=3,\n",
    "    seed_worker=functools.partial(\n",
    "        torch_utils.seed_worker, state=cs_efg_config[\"RANDOM_SEED\"]\n",
    "    ),\n",
    "    generator=torch.Generator().manual_seed(cs_efg_config[\"RANDOM_SEED\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleGNN_EFG(\n",
      "  (conv1): GCNConv(-1, 64)\n",
      "  (conv2): GCNConv(-1, 64)\n",
      "  (act1): ReLU()\n",
      "  (act2): ReLU()\n",
      "  (lin_out): Linear(-1, 1, bias=True)\n",
      "  (probs_out): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 5889\n"
     ]
    }
   ],
   "source": [
    "class SimpleHigherOrderGNN_EFG(GraphModel):\n",
    "    \"\"\"Implementation of a Graph Convolutional Network as in Adams et al. (2022)\"\"\"\n",
    "\n",
    "    # SimpleGNN_EFG(64, 1): 0.4382 MAE (test), 6k params\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_channels: int = 64,\n",
    "        out_channels: int = 1,\n",
    "        regression_target: bool = True,\n",
    "        graph_level_prediction: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = pygnn.GraphConv(-1, hidden_channels)\n",
    "        self.act1 = nn.PReLU()\n",
    "        self.pool1 = lambda x, batch: x\n",
    "        if graph_level_prediction:\n",
    "            self.pool1 = pygnn.global_mean_pool\n",
    "        self.lin_out = pygnn.Linear(-1, out_channels)\n",
    "        self.probs_out = lambda x: x\n",
    "        if not regression_target:\n",
    "            self.probs_out = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.act1(x)\n",
    "        x = self.pool1(x, batch)\n",
    "        x = self.lin_out(x)\n",
    "        return self.probs_out(x)\n",
    "    \n",
    "model = SimpleHigherOrderGNN_EFG(\n",
    "    hidden_channels=64,\n",
    "    out_channels=1,\n",
    "    regression_target=cs_efg_config[\"regression_task\"],\n",
    "    graph_level_prediction=cs_efg_config[\"graph_level_prediction\"],\n",
    ")\n",
    "# pretrained_state_dict = torch.load(\"models/runs/GraphConvNet_20230718_13h59m/state_dict_epoch6.pt\")\n",
    "# model.load_state_dict(pretrained_state_dict)\n",
    "\n",
    "# cs_efg_config[\"verbose\"] = False\n",
    "# Print summary of data and model\n",
    "if cs_efg_config[\"verbose\"]:\n",
    "    print(model)\n",
    "    with torch.no_grad():  # Initialize lazy modules, s.t. we can count its parameters.\n",
    "        batch = next(iter(train_loader))\n",
    "        batch.to(cs_efg_config[\"device\"])\n",
    "        model.to(cs_efg_config[\"device\"])\n",
    "        out = model(batch.x.float(), batch.edge_index, batch.batch)\n",
    "        print(f\"Number of parameters: {torch_utils.count_parameters(model)}\")\n",
    "        del batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING CONFIGURATION\n",
    "# cs_efg_config[\"device\"] = torch.device(\"cpu\")\n",
    "# Define the loss function with weight\n",
    "# # dataloader = DataLoader(ds_train, ds_train.size)\n",
    "# # batch = next(iter(dataloader))\n",
    "# # class_distribution = pd.DataFrame(batch.y).value_counts(normalize=True).sort_index()\n",
    "# # loss_weights = 1 / torch.tensor(class_distribution.values)\n",
    "# cs_efg_config[\"class_distribution\"] = {\n",
    "#     0.0: 0.5,\n",
    "#     1.0: 0.5 / 6,\n",
    "#     2.0: 0.5 / 6,\n",
    "#     3.0: 0.5 / 6,\n",
    "#     4.0: 0.5 / 6,\n",
    "#     5.0: 0.5 / 6,\n",
    "#     6.0: 0.5 / 6,\n",
    "# }\n",
    "# loss_weights = 1 / torch.tensor(list(cs_efg_config[\"class_distribution\"].values()))\n",
    "# cs_efg_config[\"loss_fn\"] = torch.nn.CrossEntropyLoss(\n",
    "#     weight=loss_weights.to(cs_efg_config[\"device\"])\n",
    "# )\n",
    "# # del dataloader\n",
    "# # del batch\n",
    "\n",
    "cs_efg_config[\"model_output_path\"] = \"models/CS/efg\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%Hh%Mm\")\n",
    "model_path_base = (\n",
    "    f\"{cs_efg_config['model_output_path']}/{str(model).split('(')[0]}_{timestamp}\"\n",
    ")\n",
    "# cs_efg_config[\"optimizer_settings\"] = {\n",
    "#     \"lr\": 5e-4,\n",
    "#     \"betas\": (0.9, 0.999),\n",
    "#     \"eps\": 1e-08,\n",
    "#     \"weight_decay\": 0,\n",
    "#     \"amsgrad\": False,\n",
    "# }\n",
    "# cs_efg_config[\"loss_fn\"] = torch.nn.CrossEntropyLoss()\n",
    "# cs_efg_config[\"EPOCHS\"] = 30\n",
    "# cs_efg_config[\"early_stopping\"] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "print(\"Training started, progress available in Tensorboard\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "best_state_dict_path = training_utils.run_training(\n",
    "    num_epochs=cs_efg_config[\"EPOCHS\"],\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=val_loader,\n",
    "    optimizer=cs_efg_config[\"optimizer\"](\n",
    "        model.parameters(), **cs_efg_config[\"optimizer_settings\"]\n",
    "    ),\n",
    "    loss_fn=cs_efg_config[\"loss_fn\"],\n",
    "    early_stopping_criterion=cs_efg_config[\"early_stopping\"],\n",
    "    model_path_base=model_path_base,\n",
    "    x_dtype=cs_efg_config[\"features_dtype\"],\n",
    "    y_dtype=cs_efg_config[\"target_dtype\"],\n",
    "    device=cs_efg_config[\"device\"],\n",
    "    verbose=cs_efg_config[\"verbose\"],\n",
    ")\n",
    "# Write experiment settings as JSON into model path (of the model we've just trained)\n",
    "with open(os.path.join(model_path_base, \"experiment_settings.json\"), \"w\") as file_path:\n",
    "    json.dump(evaluation_utils.get_json_serializable_dict(cs_efg_config), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_path = f\"{cs_efg_config['model_output_path']}/HigherOrderGNN_EFG_20230726_14h50m/state_dict_epoch11.pt\"  #  validation F1 | 7k params\n",
    "state_dict_path = f\"{cs_efg_config['model_output_path']}/HigherOrderGNN_EFG_20230727_14h23m/state_dict_epoch2.pt\"  #  validation F1 | 7k params\n",
    "state_dict_path = f\"{cs_efg_config['model_output_path']}/SimpleGNN_EFG_20230729_12h43m/state_dict_epoch11.pt\"  # 0.54 test MAE | 6k params\n",
    "\n",
    "# Get model evaluation report\n",
    "evaluation_report = evaluation_utils.get_best_model_evaluation(\n",
    "    model_state_dict_path=best_state_dict_path,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    model=model,\n",
    "    evaluation_reporter=evaluation_utils.get_evaluation,\n",
    "    regression=True,\n",
    "    # classification=True,\n",
    "    verbose=cs_efg_config[\"verbose\"],\n",
    ")\n",
    "\n",
    "# Store model results as JSON into model path\n",
    "with open(os.path.join(model_path_base, \"evaluation_report.json\"), \"w\") as file_path:\n",
    "    json.dump(evaluation_utils.get_json_serializable_dict(evaluation_report), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print evaluation report\n",
    "pprint.pprint(evaluation_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
