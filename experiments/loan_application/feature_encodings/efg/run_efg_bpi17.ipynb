{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/tim/Development/OCPPM'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "go_up_n_directories = lambda path, n: os.path.abspath(\n",
    "    os.path.join(*([os.path.dirname(path)] + [\"..\"] * n))\n",
    ")\n",
    "os.chdir(go_up_n_directories(os.getcwd(), 3))  # run once (otherwise restart kernel)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: Intel(R) Core(TM) i5-7500 CPU @ 3.40GHz (4x)\n",
      "Total CPU memory: 46.93GB\n",
      "Available CPU memory: 35.92GB\n",
      "GPU: NVIDIA GeForce GTX 960\n",
      "Total GPU memory: 4096.0MB\n",
      "Available GPU memory: 4029.0MB\n",
      "Platform: Linux-5.19.0-46-generic-x86_64-with-glibc2.35\n"
     ]
    }
   ],
   "source": [
    "# DEPENDENCIES\n",
    "# Python native\n",
    "import os\n",
    "import pickle\n",
    "import pprint\n",
    "import random\n",
    "import functools\n",
    "import json\n",
    "import time\n",
    "from copy import copy\n",
    "from datetime import datetime\n",
    "from statistics import median as median\n",
    "from sys import platform\n",
    "from typing import Any, Callable\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import ocpa.algo.predictive_monitoring.factory as feature_factory\n",
    "\n",
    "# PyG\n",
    "import torch\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "import torch.utils.tensorboard\n",
    "\n",
    "# Object centric process mining\n",
    "from ocpa.algo.predictive_monitoring.obj import Feature_Storage as FeatureStorage\n",
    "\n",
    "# # Simple machine learning models, procedure tools, and evaluation metrics\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "from tqdm import tqdm\n",
    "from torch import tensor\n",
    "\n",
    "# Custom imports\n",
    "from experiments.efg import EFG\n",
    "from experiments.efg_sg import EFG_SG\n",
    "from utilities import torch_utils\n",
    "from utilities import data_utils\n",
    "from utilities import training_utils\n",
    "from utilities import evaluation_utils\n",
    "\n",
    "# from importing_ocel import build_feature_storage, load_ocel, pickle_feature_storage\n",
    "from models.definitions.geometric_models import (\n",
    "    AGNN_EFG,\n",
    "    AdamsGCN,\n",
    "    GraphModel,\n",
    "    HigherOrderGNN_EFG,\n",
    "    SimpleGNN_EFG,\n",
    ")\n",
    "import torch_geometric.nn as pygnn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as O\n",
    "import torch.nn as nn\n",
    "\n",
    "# Print system info\n",
    "torch_utils.print_system_info()\n",
    "\n",
    "# Setup\n",
    "bpi17_efg_config = {\n",
    "    \"model_output_path\": \"models/BPI17/efg\",\n",
    "    \"STORAGE_PATH\": \"data/BPI17/feature_encodings/EFG/efg\",\n",
    "    \"SPLIT_FEATURE_STORAGE_FILE\": \"BPI_split_[C2_P2_P3_P5_O3_Action_EventOrigin_OrgResource].fs\",\n",
    "    \"TARGET_LABEL\": (feature_factory.EVENT_REMAINING_TIME, ()),\n",
    "    \"regression_task\": True,\n",
    "    \"graph_level_prediction\": True,\n",
    "    \"features_dtype\": torch.float32,\n",
    "    \"target_dtype\": torch.float32,\n",
    "    \"SUBGRAPH_SIZE\": 4,\n",
    "    \"BATCH_SIZE\": 64,\n",
    "    \"RANDOM_SEED\": 42,\n",
    "    \"EPOCHS\": 30,\n",
    "    \"early_stopping\": 4,\n",
    "    \"hidden_dim\": 16,\n",
    "    \"optimizer\": O.Adam,\n",
    "    \"optimizer_settings\": {\n",
    "        \"lr\": 0.001,\n",
    "        \"betas\": (0.9, 0.999),\n",
    "        \"eps\": 1e-08,\n",
    "        \"weight_decay\": 0,\n",
    "        \"amsgrad\": False,\n",
    "    },\n",
    "    \"loss_fn\": torch.nn.L1Loss(),\n",
    "    \"verbose\": True,\n",
    "    \"track_time\": True,\n",
    "    \"skip_cache\": False,\n",
    "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}\n",
    "\n",
    "# ADAPTATIONS\n",
    "# bpi17_efg_config[\"optimizer_settings\"] = {\n",
    "#     \"lr\": 5e-4,\n",
    "#     \"betas\": (0.9, 0.999),\n",
    "#     \"eps\": 1e-08,\n",
    "#     \"weight_decay\": 0,\n",
    "#     \"amsgrad\": False,\n",
    "# }\n",
    "# bpi17_hoeg_config[\"loss_fn\"] = torch.nn.L1Loss()\n",
    "# bpi17_efg_config[\"BATCH_SIZE\"] = 64\n",
    "# bpi17_efg_config[\"EPOCHS\"] = 30\n",
    "# bpi17_efg_config[\"early_stopping\"] = 5\n",
    "# bpi17_efg_config['skip_cache']=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/Development/OCPPM/.env/lib/python3.9/site-packages/sklearn/base.py:324: UserWarning: Trying to unpickle estimator StandardScaler from version 0.24.2 when using version 1.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Get data and dataloaders\n",
    "ds_train, ds_val, ds_test = data_utils.load_datasets(\n",
    "    dataset_class=EFG_SG,\n",
    "    storage_path=bpi17_efg_config[\"STORAGE_PATH\"],\n",
    "    split_feature_storage_file=bpi17_efg_config[\"SPLIT_FEATURE_STORAGE_FILE\"],\n",
    "    target_label=bpi17_efg_config[\"TARGET_LABEL\"],\n",
    "    graph_level_target=bpi17_efg_config[\"graph_level_prediction\"],\n",
    "    features_dtype=bpi17_efg_config[\"features_dtype\"],\n",
    "    target_dtype=bpi17_efg_config[\"target_dtype\"],\n",
    "    subgraph_size=bpi17_efg_config[\"SUBGRAPH_SIZE\"],\n",
    "    train=True,\n",
    "    val=True,\n",
    "    test=True,\n",
    "    skip_cache=bpi17_efg_config[\"skip_cache\"],\n",
    ")\n",
    "train_loader, val_loader, test_loader = data_utils.prepare_dataloaders(\n",
    "    batch_size=bpi17_efg_config[\"BATCH_SIZE\"],\n",
    "    ds_train=ds_train,\n",
    "    ds_val=ds_val,\n",
    "    ds_test=ds_test,\n",
    "    seed_worker=functools.partial(\n",
    "        torch_utils.seed_worker, state=bpi17_efg_config[\"RANDOM_SEED\"]\n",
    "    ),\n",
    "    generator=torch.Generator().manual_seed(bpi17_efg_config[\"RANDOM_SEED\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HigherOrderGNN_EFG(\n",
      "  (conv1): GraphConv(-1, 16)\n",
      "  (conv2): GraphConv(-1, 16)\n",
      "  (act1): PReLU(num_parameters=1)\n",
      "  (act2): PReLU(num_parameters=1)\n",
      "  (lin_out): Linear(-1, 1, bias=True)\n",
      ")\n",
      "Number of parameters: 1427\n"
     ]
    }
   ],
   "source": [
    "# # MODEL INITIALIZATION\n",
    "# model = HigherOrderGNN_EFG(bpi17_efg_config['hidden_dim'], 1)\n",
    "# # pretrained_state_dict = torch.load(\"models/runs/GraphConvNet_20230718_13h59m/state_dict_epoch6.pt\")\n",
    "# # model.load_state_dict(pretrained_state_dict)\n",
    "# model.to(bpi17_efg_config[\"device\"])\n",
    "\n",
    "# # Print summary of data and model\n",
    "# if bpi17_efg_config[\"verbose\"]:\n",
    "#     print(model)\n",
    "#     with torch.no_grad():  # Initialize lazy modules, s.t. we can count its parameters.\n",
    "#         batch = next(iter(train_loader))\n",
    "#         batch.to(bpi17_efg_config[\"device\"])\n",
    "#         out = model(batch.x.float(), batch.edge_index, batch.batch)\n",
    "#         print(f\"Number of parameters: {torch_utils.count_parameters(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TRAINING\n",
    "# print(\"Training started, progress available in Tensorboard\")\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d_%Hh%Mm\")\n",
    "# model_path_base = f\"models/BPI17/efg/lr={bpi17_efg_config['optimizer_settings']['lr']}_hidden_dim={hidden_dim}/{str(model).split('(')[0]}_{timestamp}\"\n",
    "\n",
    "# best_state_dict_path = training_utils.run_training(\n",
    "#     num_epochs=bpi17_efg_config[\"EPOCHS\"],\n",
    "#     model=model,\n",
    "#     train_loader=train_loader,\n",
    "#     validation_loader=val_loader,\n",
    "#     optimizer=bpi17_efg_config[\"optimizer\"](\n",
    "#         model.parameters(), **bpi17_efg_config[\"optimizer_settings\"]\n",
    "#     ),\n",
    "#     loss_fn=bpi17_efg_config[\"loss_fn\"],\n",
    "#     early_stopping_criterion=bpi17_efg_config[\"early_stopping\"],\n",
    "#     model_path_base=model_path_base,\n",
    "#     x_dtype=bpi17_efg_config[\"features_dtype\"],\n",
    "#     y_dtype=bpi17_efg_config[\"target_dtype\"],\n",
    "#     device=bpi17_efg_config[\"device\"],\n",
    "#     verbose=True,\n",
    "# )\n",
    "# # Write experiment settings as JSON into model path (of the model we've just trained)\n",
    "# with open(os.path.join(model_path_base, \"experiment_settings.json\"), \"w\") as file_path:\n",
    "#     json.dump(evaluation_utils.get_json_serializable_dict(bpi17_efg_config), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2626/2626 [01:16<00:00, 34.35it/s]\n",
      "100%|██████████| 657/657 [00:18<00:00, 36.15it/s]\n",
      "100%|██████████| 699/699 [00:28<00:00, 24.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test': {'report': {'MAE': 0.4087477,\n",
      "                     'MAPE': 2.7260685,\n",
      "                     'MSE': 0.4682943,\n",
      "                     'R^2': -0.015702805917782614}},\n",
      " 'Train': {'report': {'MAE': 0.4119272,\n",
      "                      'MAPE': 6.021801,\n",
      "                      'MSE': 0.48191255,\n",
      "                      'R^2': -0.0268219392311162}},\n",
      " 'Validation': {'report': {'MAE': 0.42308843,\n",
      "                           'MAPE': 4.256179,\n",
      "                           'MSE': 0.49531737,\n",
      "                           'R^2': -0.07168010063702734}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # Evaluation\n",
    "# state_dict_path = \"models/BPI17/efg/AGNN_20230714_12h19m\"  # 0.59 test mae\n",
    "# state_dict_path = \"models/BPI17/efg/AGNN_20230714_14h26m\"  # 0.54 test mae\n",
    "# state_dict_path = \"models/BPI17/efg/AGNN_20230717_15h16m\"  # 0.48 test mae ()\n",
    "# state_dict_path = \"models/BPI17/efg/AGNN_20230717_16h37m\"  # 0.47 test mae\n",
    "# state_dict_path = \"models/BPI17/efg/AGNN_20230717_15h51m\"  # 0.4557 test mae (ChebConv)\n",
    "# state_dict_path = \"models/BPI17/efg/AGNN_20230717_16h58m\"  # 0.4546 test mae\n",
    "# state_dict_path = \"models/BPI17/efg/AGNN_20230717_23h22m\"  # 0.4534 test mae\n",
    "# state_dict_path = (\n",
    "#     \"models/BPI17/efg/SimpleGNN_20230718_09h30m\"  # 0.4382 test mae | 6k params\n",
    "# )\n",
    "# state_dict_path = (\n",
    "#     \"models/BPI17/efg/TransformerGNN_20230718_09h46m\"  # 0.4290 test mae | 24k params\n",
    "# )\n",
    "# state_dict_path = (\n",
    "#     \"models/BPI17/efg/GraphConvArch_20230718_10h08m\"  # 0.4248 test mae | 12k params\n",
    "# )\n",
    "# state_dict_path = (\n",
    "#     \"models/BPI17/efg/GraphConvNet_20230718_11h35m\"  # 0.4149 test mae | 7k params\n",
    "# )\n",
    "# state_dict_path = (\n",
    "#     \"models/BPI17/efg/GraphConvNet_20230718_11h54m\"  # 0.4113 test mae | 7k params\n",
    "# )\n",
    "# state_dict_path = \"models/BPI17/efg/GraphConvNet_20230718_13h59m\"  # 0.4040 test mae | 7k params | fine-tuning pretrained 'GraphConvNet_20230718_11h54m'  // best so far!\n",
    "# state_dict_path = (\n",
    "#     \"models/BPI17/efg/HigherOrderGNN_EFG_20230720_13h11m\"  # 0.4087 test mae | 7k params\n",
    "# )\n",
    "\n",
    "# # Get model evaluation report\n",
    "# evaluation_report = evaluation_utils.get_best_model_evaluation(\n",
    "#     model_state_dict_path=best_state_dict_path,\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     test_loader=test_loader,\n",
    "#     model=model,\n",
    "#     evaluation_reporter=evaluation_utils.get_evaluation,\n",
    "#     regression=True,\n",
    "#     verbose=bpi17_efg_config[\"verbose\"],\n",
    "#     track_time=bpi17_efg_config[\"track_time\"],\n",
    "# )\n",
    "\n",
    "# # Store model results as JSON into model path\n",
    "# with open(os.path.join(model_path_base, \"evaluation_report.json\"), \"w\") as file_path:\n",
    "#     json.dump(evaluation_utils.get_json_serializable_dict(evaluation_report), file_path)\n",
    "\n",
    "# # Print evaluation report\n",
    "# pprint.pprint(evaluation_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_configuration(\n",
    "    model_class: GraphModel,\n",
    "    lr: float,\n",
    "    hidden_dim: int,\n",
    "    device: torch.device = None,\n",
    "    track_time: bool = True,\n",
    "    verbose: bool = False,\n",
    ") -> None:\n",
    "    if device:\n",
    "        bpi17_efg_config[\"device\"] = device\n",
    "    bpi17_efg_config[\"verbose\"] = verbose\n",
    "    bpi17_efg_config[\"track_time\"] = track_time\n",
    "    # HYPERPARAMETER INITIALIZATION (those that we tune)\n",
    "    bpi17_efg_config[\"hidden_dim\"] = hidden_dim\n",
    "    bpi17_efg_config[\"optimizer_settings\"][\"lr\"] = lr\n",
    "    # MODEL INITIALIZATION\n",
    "    model = model_class(hidden_dim, 1)\n",
    "    # pretrained_state_dict = torch.load(\"models/runs/GraphConvNet_20230718_13h59m/state_dict_epoch6.pt\")\n",
    "    # model.load_state_dict(pretrained_state_dict)\n",
    "    model.to(bpi17_efg_config[\"device\"])\n",
    "\n",
    "    # TRAINING\n",
    "    if verbose:\n",
    "        print(\"Training started, progress available in Tensorboard\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    start_train_time = datetime.now()\n",
    "    timestamp = start_train_time.strftime(\"%Y%m%d_%Hh%Mm\")\n",
    "    model_path_base = f\"{bpi17_efg_config['model_output_path']}/lr={lr}_hidden_dim={hidden_dim}/{str(model).split('(')[0]}_{timestamp}\"\n",
    "\n",
    "    best_state_dict_path = training_utils.run_training(\n",
    "        num_epochs=bpi17_efg_config[\"EPOCHS\"],\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        validation_loader=val_loader,\n",
    "        optimizer=bpi17_efg_config[\"optimizer\"](\n",
    "            model.parameters(), **bpi17_efg_config[\"optimizer_settings\"]\n",
    "        ),\n",
    "        loss_fn=bpi17_efg_config[\"loss_fn\"],\n",
    "        early_stopping_criterion=bpi17_efg_config[\"early_stopping\"],\n",
    "        model_path_base=model_path_base,\n",
    "        x_dtype=bpi17_efg_config[\"features_dtype\"],\n",
    "        y_dtype=bpi17_efg_config[\"target_dtype\"],\n",
    "        device=bpi17_efg_config[\"device\"],\n",
    "        verbose=bpi17_efg_config[\"verbose\"],\n",
    "    )\n",
    "\n",
    "    total_train_time = datetime.now() - start_train_time\n",
    "    # Write experiment settings as JSON into model path (of the model we've just trained)\n",
    "    with open(\n",
    "        os.path.join(model_path_base, \"experiment_settings.json\"), \"w\"\n",
    "    ) as file_path:\n",
    "        json.dump(\n",
    "            evaluation_utils.get_json_serializable_dict(bpi17_efg_config),\n",
    "            file_path,\n",
    "            indent=2,\n",
    "        )\n",
    "\n",
    "    # EVALUATION\n",
    "    # Get model evaluation report\n",
    "    evaluation_report = evaluation_utils.get_best_model_evaluation(\n",
    "        model_state_dict_path=best_state_dict_path,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        model=model,\n",
    "        evaluation_reporter=evaluation_utils.get_evaluation,\n",
    "        regression=True,\n",
    "        verbose=bpi17_efg_config[\"verbose\"],\n",
    "        track_time=bpi17_efg_config[\"track_time\"],\n",
    "    )\n",
    "    evaluation_report[\"Train\"][\"report\"][\"training_time\"] = total_train_time\n",
    "    # Store model results as JSON into model path\n",
    "    model_architecture = torch_utils.parse_model_string(model)\n",
    "    model_architecture[\"Number of parameters\"] = torch_utils.count_parameters(model)\n",
    "    with open(\n",
    "        os.path.join(model_path_base, \"model_architecture.json\"), \"w\"\n",
    "    ) as file_path:\n",
    "        json.dump(model_architecture, file_path, indent=2)\n",
    "\n",
    "    with open(\n",
    "        os.path.join(model_path_base, \"evaluation_report.json\"), \"w\"\n",
    "    ) as file_path:\n",
    "        json.dump(\n",
    "            evaluation_utils.get_json_serializable_dict(evaluation_report),\n",
    "            file_path,\n",
    "            indent=2,\n",
    "        )\n",
    "\n",
    "    # Print evaluation report\n",
    "    print(f\"lr={lr}, hidden_dim={hidden_dim}:\")\n",
    "    print(f\"    {model_architecture['Number of parameters']} parameters\")\n",
    "    print(f\"    {evaluation_report['Train']['report']['training_time']} H:m:s\")\n",
    "    print(f\"    {evaluation_report['Test']['report']['MAE']:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping after 18 epochs.\n",
      "lr=0.01, hidden_dim=8:\n",
      "    587 parameters\n",
      "    0:11:19.402490 H:m:s\n",
      "    0.4284\n",
      "\n",
      "Early stopping after 6 epochs.\n",
      "lr=0.01, hidden_dim=16:\n",
      "    1427 parameters\n",
      "    0:03:57.227688 H:m:s\n",
      "    0.4321\n",
      "\n",
      "Early stopping after 10 epochs.\n",
      "lr=0.01, hidden_dim=24:\n",
      "    2523 parameters\n",
      "    0:06:36.402405 H:m:s\n",
      "    0.4238\n",
      "\n",
      "Early stopping after 15 epochs.\n",
      "lr=0.01, hidden_dim=32:\n",
      "    3875 parameters\n",
      "    0:10:18.807842 H:m:s\n",
      "    0.4219\n",
      "\n",
      "Early stopping after 14 epochs.\n",
      "lr=0.01, hidden_dim=48:\n",
      "    7347 parameters\n",
      "    0:09:24.318881 H:m:s\n",
      "    0.4240\n",
      "\n",
      "Early stopping after 8 epochs.\n",
      "lr=0.01, hidden_dim=64:\n",
      "    11843 parameters\n",
      "    0:05:14.845230 H:m:s\n",
      "    0.4272\n",
      "\n",
      "Early stopping after 13 epochs.\n",
      "lr=0.01, hidden_dim=128:\n",
      "    40067 parameters\n",
      "    0:09:49.965929 H:m:s\n",
      "    0.4287\n",
      "\n",
      "Early stopping after 6 epochs.\n",
      "lr=0.01, hidden_dim=256:\n",
      "    145667 parameters\n",
      "    0:04:15.462002 H:m:s\n",
      "    0.4461\n",
      "\n",
      "Early stopping after 27 epochs.\n",
      "lr=0.001, hidden_dim=8:\n",
      "    587 parameters\n",
      "    0:21:30.648002 H:m:s\n",
      "    0.4303\n",
      "\n",
      "lr=0.001, hidden_dim=16:\n",
      "    1427 parameters\n",
      "    0:18:29.603920 H:m:s\n",
      "    0.4149\n",
      "\n",
      "lr=0.001, hidden_dim=24:\n",
      "    2523 parameters\n",
      "    0:18:06.767217 H:m:s\n",
      "    0.4135\n",
      "\n",
      "Early stopping after 15 epochs.\n",
      "lr=0.001, hidden_dim=32:\n",
      "    3875 parameters\n",
      "    0:10:35.929280 H:m:s\n",
      "    0.4142\n",
      "\n",
      "Early stopping after 26 epochs.\n",
      "lr=0.001, hidden_dim=48:\n",
      "    7347 parameters\n",
      "    0:17:56.468000 H:m:s\n",
      "    0.4075\n",
      "\n",
      "Early stopping after 30 epochs.\n",
      "lr=0.001, hidden_dim=64:\n",
      "    11843 parameters\n",
      "    0:20:11.086467 H:m:s\n",
      "    0.4064\n",
      "\n",
      "Early stopping after 24 epochs.\n",
      "lr=0.001, hidden_dim=128:\n",
      "    40067 parameters\n",
      "    0:17:06.949516 H:m:s\n",
      "    0.4076\n",
      "\n",
      "Early stopping after 14 epochs.\n",
      "lr=0.001, hidden_dim=256:\n",
      "    145667 parameters\n",
      "    0:11:27.485126 H:m:s\n",
      "    0.4101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_range = [0.01, 0.001]\n",
    "hidden_dim_range = [8, 16, 24, 32, 48, 64, 128, 256]\n",
    "for lr in lr_range:\n",
    "    for hidden_dim in hidden_dim_range:\n",
    "        run_experiment_configuration(\n",
    "            model_class=HigherOrderGNN_EFG,\n",
    "            lr=lr,\n",
    "            hidden_dim=hidden_dim,\n",
    "            track_time=True,\n",
    "            verbose=False,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
